{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import time\n",
    "from datetime import timedelta\n",
    "from temporalcontext import settings\n",
    "from temporalcontext.functions import read_selmap, read_folds_info\n",
    "\n",
    "\n",
    "# Given the use LoG operator for input conditioning, it's better start\n",
    "# with a lower learning rate.\n",
    "def variable_learning_rate(epoch):\n",
    "    var_rates = [0.001 * factor for factor in [1.0, 1e-1, 1e-2, 1e-3]]\n",
    "    for idx, boundary in enumerate([10, 35, 45]):\n",
    "        if epoch <= boundary:\n",
    "            return var_rates[idx]\n",
    "    return var_rates[-1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "selmap = read_selmap(os.path.join(settings.raw_data_root, 'selmap.csv'))\n",
    "fold_file_idxs = read_folds_info(os.path.join(settings.raw_data_root, 'folds_info.txt'))\n",
    "\n",
    "# Loop over each unique segment advance\n",
    "for seg_adv, ts in list(set([(ex['segment_advance'], ex['time_steps']) for ex in settings.lstm_experiments])):\n",
    "    \n",
    "    random_state = np.random.RandomState(settings.random_seed)\n",
    "\n",
    "    segments_root = os.path.join(settings.project_root,\n",
    "                                 settings.segments_dir,\n",
    "                                 'seg_adv_{:.2f}'.format(seg_adv))\n",
    "    \n",
    "    train_times = list()\n",
    "    \n",
    "    for fold_idx, fold_info in enumerate(fold_file_idxs):\n",
    "        print('===== Fold {:02d}, segment advance {:.2f} s ====='.format(fold_idx + 1, seg_adv))\n",
    "\n",
    "        # Load all positive and negative training samples for the fold\n",
    "        pos_samples = list()\n",
    "        neg_samples = list()\n",
    "        for f_idx in fold_info['train']:\n",
    "            au_file = selmap[f_idx][0]\n",
    "            \n",
    "            with np.load(os.path.join(segments_root, settings.class_dirs[0], au_file + '.npz'), 'r') as data:\n",
    "                pos_samples.append(data['segments'])\n",
    "                \n",
    "            with np.load(os.path.join(segments_root, settings.class_dirs[1], au_file + '.npz'), 'r') as data:\n",
    "                neg_samples.append(data['segments'])\n",
    "        \n",
    "        pos_samples = np.concatenate(pos_samples, axis=0)\n",
    "        neg_samples = np.concatenate(neg_samples, axis=0)\n",
    "        \n",
    "        # Shuffle the indices for random splitting into train & eval subsets.\n",
    "        # Then, restrict to limits.\n",
    "        pos_samples_idxs = random_state.permutation(pos_samples.shape[0])[:settings.max_per_class_training_samples]\n",
    "        neg_samples_idxs = random_state.permutation(neg_samples.shape[0])[:settings.max_per_class_training_samples]\n",
    "        \n",
    "        num_pos_samples = len(pos_samples_idxs)\n",
    "        num_neg_samples = len(neg_samples_idxs)\n",
    "        total_samples = num_pos_samples + num_neg_samples\n",
    "\n",
    "        # Identify the points where each class' samples are to be split\n",
    "        pos_eval_split = int(round((1.0 - settings.validation_split) * num_pos_samples))\n",
    "        neg_eval_split = int(round((1.0 - settings.validation_split) * num_neg_samples))\n",
    "        \n",
    "        num_train_samples = pos_eval_split + neg_eval_split\n",
    "        \n",
    "        class_weights = {\n",
    "            0: num_train_samples / (2 * neg_eval_split),\n",
    "            1: num_train_samples / (2 * pos_eval_split)\n",
    "        }\n",
    "\n",
    "        # Combine train/eval pos & neg samples splits\n",
    "        train_data_x = np.concatenate([pos_samples[pos_samples_idxs[:pos_eval_split], ...],\n",
    "                                       neg_samples[neg_samples_idxs[:neg_eval_split], ...]], axis=0)\n",
    "        train_data_y = np.concatenate([np.repeat([[0, 1]], pos_eval_split, axis=0),\n",
    "                                       np.repeat([[1, 0]], neg_eval_split, axis=0)], axis=0)\n",
    "        eval_data_x = np.concatenate([pos_samples[pos_samples_idxs[pos_eval_split:], ...],\n",
    "                                      neg_samples[neg_samples_idxs[neg_eval_split:], ...]], axis=0)\n",
    "        eval_data_y = np.concatenate([np.repeat([[0, 1]], num_pos_samples - pos_eval_split, axis=0),\n",
    "                                      np.repeat([[1, 0]], num_neg_samples - neg_eval_split, axis=0)], axis=0)\n",
    "        \n",
    "        # Add channel dimension\n",
    "        train_data_x = np.expand_dims(train_data_x, axis=3)\n",
    "        eval_data_x = np.expand_dims(eval_data_x, axis=3)\n",
    "        \n",
    "        print('Positive samples : {:7d} training, {:5d} eval'.format(\n",
    "            pos_eval_split, num_pos_samples - pos_eval_split))\n",
    "        print('Negative samples : {:7d} training, {:5d} eval'.format(\n",
    "            neg_eval_split, num_neg_samples - neg_eval_split))\n",
    "        print('Total samples    : {:7d} training, {:5d} eval'.format(\n",
    "            train_data_x.shape[0], eval_data_x.shape[0]))\n",
    "        \n",
    "        val_steps = np.ceil(eval_data_x.shape[0] / settings.batch_size).astype(np.int)\n",
    "        \n",
    "        # Shuffle training group so that the classes aren't lumped together\n",
    "        shuffle_idxs = random_state.permutation(train_data_x.shape[0])\n",
    "        \n",
    "        tf.keras.backend.clear_session()\n",
    "        tf.random.set_seed(settings.random_seed)\n",
    "        \n",
    "        train_dataset = tf.data.Dataset.from_tensor_slices((train_data_x[shuffle_idxs, ...],\n",
    "                                                            train_data_y[shuffle_idxs, ...]))\n",
    "        train_dataset = train_dataset.cache().shuffle(settings.buffer_size).batch(settings.batch_size)\n",
    "\n",
    "        eval_dataset = tf.data.Dataset.from_tensor_slices((eval_data_x, eval_data_y))\n",
    "        eval_dataset = eval_dataset.batch(settings.batch_size).repeat()\n",
    "        \n",
    "        # Load the model architecture\n",
    "        with open('baseCNN_architecture.json', 'r') as json_file:\n",
    "            classifier = tf.keras.models.model_from_json(json_file.read())\n",
    "        \n",
    "        #classifier.summary()\n",
    "        \n",
    "        classifier.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=variable_learning_rate(0)),\n",
    "                           loss=tf.keras.losses.CategoricalCrossentropy(),\n",
    "                           metrics=[tf.keras.metrics.CategoricalAccuracy()])\n",
    "\n",
    "        # Train\n",
    "        start_time = time.time()\n",
    "        history = classifier.fit(\n",
    "            x=train_dataset,\n",
    "            epochs=settings.epochs,\n",
    "            validation_data=eval_dataset,\n",
    "            validation_freq=settings.epochs_between_evals,\n",
    "            validation_steps=val_steps,\n",
    "            class_weight=class_weights,\n",
    "            initial_epoch=0,\n",
    "            shuffle=False,\n",
    "            callbacks=[tf.keras.callbacks.LearningRateScheduler(variable_learning_rate)],\n",
    "            verbose=2)\n",
    "        end_time = time.time()\n",
    "\n",
    "        train_times.append(end_time - start_time)\n",
    "        \n",
    "        # Save trained model. Reset metrics before saving\n",
    "        classifier.reset_metrics()\n",
    "        model_dir = os.path.join(settings.project_root, settings.folds_dir,\n",
    "                                 'f{:02d}'.format(fold_idx + 1), 'seg_adv_{:.2f}'.format(seg_adv),\n",
    "                                 settings.models_dir)\n",
    "        os.makedirs(model_dir, exist_ok=True)\n",
    "        classifier.save(os.path.join(model_dir, 'baseCNN.h5'))\n",
    "\n",
    "        # Free up some memory before next iteration\n",
    "        del classifier, train_dataset, eval_dataset, train_data_x, train_data_y, eval_data_x, eval_data_y\n",
    "        \n",
    "    print('----------------------------------------')\n",
    "    print('Training times for segment advance {:.2f} s:'.format(seg_adv))\n",
    "    print('    Min = {}'.format(timedelta(seconds=min(train_times))))\n",
    "    print('    Max = {}'.format(timedelta(seconds=max(train_times))))\n",
    "    print('    Avg = {}'.format(timedelta(seconds=sum(train_times) / len(train_times))))\n",
    "    print('========================================')\n",
    "    print()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
