{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import time\n",
    "from datetime import timedelta\n",
    "\n",
    "from temporalcontext import settings\n",
    "from temporalcontext.functions import read_selmap, read_folds_info, \\\n",
    "    get_inputs_for_lstm, get_lstm_model_filename\n",
    "\n",
    "\n",
    "# Choose which hybrid variant to train\n",
    "#lstm_type = 't1'   # Uses only scores from CNN\n",
    "#lstm_type = 't2'   # Uses only embeddings from CNN\n",
    "lstm_type = 't3'   # Uses scores & embeddings (concatenated) from the CNN\n",
    "\n",
    "# NOTE: Set this to None if there were no \"secondary\" annotations in the dataset\n",
    "secondary_annots_path = os.path.join(settings.raw_data_root, settings.added_annot_dir)\n",
    "\n",
    "\n",
    "def variable_learning_rate(epoch):\n",
    "    var_rates = [0.001 * factor for factor in [1.0, 1/3]]\n",
    "    for idx, boundary in enumerate([35]):\n",
    "        if epoch <= boundary:\n",
    "            return var_rates[idx]\n",
    "    return var_rates[-1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "selmap = read_selmap(os.path.join(settings.raw_data_root, 'selmap.csv'))\n",
    "fold_file_idxs = read_folds_info(os.path.join(settings.raw_data_root, 'folds_info.txt'))\n",
    "\n",
    "train_times = dict()\n",
    "\n",
    "for lstm_exp in settings.lstm_experiments:\n",
    "\n",
    "    random_state = np.random.RandomState(settings.random_seed)\n",
    "\n",
    "    for fold_idx, fold_info in enumerate(fold_file_idxs):\n",
    "\n",
    "        print('---------- Type {:s}, segment_advance={:.2f}, PP={:d}, Fold {:02d} ----------'.format(\n",
    "            lstm_type, lstm_exp['segment_advance'], lstm_exp['pp'], fold_idx + 1))\n",
    "        \n",
    "        fold_seg_root = os.path.join(settings.project_root, settings.folds_dir,\n",
    "                                     'f{:02d}'.format(fold_idx + 1),\n",
    "                                     'seg_adv_{:.2f}'.format(lstm_exp['segment_advance']))\n",
    "\n",
    "        input_root = os.path.join(fold_seg_root, settings.lstm_data_dir)\n",
    "        model_dir = os.path.join(fold_seg_root, settings.models_dir)\n",
    "        \n",
    "        secondary_annots_info = None if secondary_annots_path is None else (\n",
    "            secondary_annots_path,\n",
    "            settings.annot_duration,\n",
    "            settings.segment_length,\n",
    "            lstm_exp['segment_advance'])\n",
    "\n",
    "        # Get data\n",
    "        song_data_x, song_data_y, nonsong_data_x, nonsong_data_y = \\\n",
    "            get_inputs_for_lstm(\n",
    "                lstm_type,\n",
    "                input_root,\n",
    "                [selmap[f_idx][0] for f_idx in fold_info['train']],\n",
    "                lstm_exp['time_steps'],\n",
    "                lstm_exp['pp'],\n",
    "                settings.section_suffixes,\n",
    "                secondary_annots_info)\n",
    "\n",
    "        total_num_samples = song_data_x.shape[0] + nonsong_data_x.shape[0]\n",
    "        pos_samples_idxs = np.where(song_data_y == 1)[0]\n",
    "        neg_samples_idxs = np.where(song_data_y != 1)[0]\n",
    "        num_nonsong_samples = nonsong_data_y.shape[0]\n",
    "        nonsong_samples_idxs = np.arange(num_nonsong_samples)\n",
    "\n",
    "        print('All available samples: {:6d} song ({:6d} Pos, {:6d} Neg), {:6d} non-song ({:6d} Other)'.format(\n",
    "            song_data_x.shape[0], len(pos_samples_idxs), len(neg_samples_idxs),\n",
    "            nonsong_data_x.shape[0], num_nonsong_samples))\n",
    "\n",
    "        # Shuffle the indices for random splitting into train & eval subsets\n",
    "        random_state.shuffle(pos_samples_idxs)\n",
    "        random_state.shuffle(neg_samples_idxs)\n",
    "        random_state.shuffle(nonsong_samples_idxs)\n",
    "\n",
    "        # Restrict to limits. Non-song samples are usually well below limit, so take all as neg.\n",
    "        pos_samples_idxs = pos_samples_idxs[:settings.max_per_class_training_samples]\n",
    "        neg_samples_idxs = neg_samples_idxs[:(settings.max_per_class_training_samples - num_nonsong_samples)]\n",
    "\n",
    "        # Update counts\n",
    "        num_pos_samples = len(pos_samples_idxs)\n",
    "        num_neg_samples = len(neg_samples_idxs)\n",
    "\n",
    "        # Identify the points where the song-pos/song-neg/nonsong-neg groups are to be split\n",
    "        pos_eval_split = int(round((1.0 - settings.validation_split) * num_pos_samples))\n",
    "        neg_eval_split = int(round((1.0 - settings.validation_split) * num_neg_samples))\n",
    "        non_song_eval_split = int(round((1.0 - settings.validation_split) * num_nonsong_samples))\n",
    "\n",
    "        num_combined_samples = num_pos_samples + num_neg_samples + num_nonsong_samples\n",
    "        num_combined_train_samples = (pos_eval_split + neg_eval_split + non_song_eval_split)\n",
    "\n",
    "        print('Pos samples  : {:7d} training, {:5d} eval'.format(\n",
    "            pos_eval_split, num_pos_samples - pos_eval_split))\n",
    "        print('Neg samples  : {:7d} ({:7d} + {:6d}) training, {:5d} ({:5d} + {:5d}) eval'.format(\n",
    "            neg_eval_split + non_song_eval_split, neg_eval_split, non_song_eval_split,\n",
    "            (num_neg_samples - neg_eval_split) + (num_nonsong_samples - non_song_eval_split),\n",
    "            num_neg_samples - neg_eval_split, num_nonsong_samples - non_song_eval_split))\n",
    "        print('Total samples: {:7d} training, {:5d} eval'.format(\n",
    "            num_combined_train_samples, num_combined_samples - num_combined_train_samples))\n",
    "        print('Input shape: {}'.format(song_data_x.shape[1:]))\n",
    "\n",
    "        train_steps = num_combined_train_samples // settings.batch_size\n",
    "        val_steps = np.ceil((num_combined_samples - num_combined_train_samples) / settings.batch_size).astype(np.int)\n",
    "\n",
    "        # Combine the song-pos/song-neg groups to form train & eval subsets\n",
    "        song_train_idxs = np.concatenate([pos_samples_idxs[:pos_eval_split], neg_samples_idxs[:neg_eval_split]])\n",
    "        song_eval_idxs = np.concatenate([pos_samples_idxs[pos_eval_split:], neg_samples_idxs[neg_eval_split:]])\n",
    "        nonsong_train_idxs = nonsong_samples_idxs[:non_song_eval_split]\n",
    "        nonsong_eval_idxs = nonsong_samples_idxs[non_song_eval_split:]\n",
    "        del pos_samples_idxs, neg_samples_idxs, nonsong_samples_idxs\n",
    "\n",
    "        # Shuffle so that all pos & all neg samples don't stay together\n",
    "        random_state.shuffle(song_train_idxs)\n",
    "        random_state.shuffle(song_eval_idxs)\n",
    "\n",
    "        # Combine train/eval splits from song and nonsong sets\n",
    "        train_data_x = np.concatenate([song_data_x[song_train_idxs, ...], nonsong_data_x[nonsong_train_idxs, ...]], axis=0)\n",
    "        train_data_y = np.concatenate([song_data_y[song_train_idxs], nonsong_data_y[nonsong_train_idxs]], axis=0)\n",
    "        eval_data_x = np.concatenate([song_data_x[song_eval_idxs, ...], nonsong_data_x[nonsong_eval_idxs, ...]], axis=0)\n",
    "        eval_data_y = np.concatenate([song_data_y[song_eval_idxs], nonsong_data_y[nonsong_eval_idxs]], axis=0)\n",
    "\n",
    "        del song_data_x, song_data_y, nonsong_data_x, nonsong_data_y\n",
    "        del song_train_idxs, song_eval_idxs, nonsong_train_idxs, nonsong_eval_idxs\n",
    "\n",
    "        class_weights = {\n",
    "            0: num_combined_train_samples / (2 * (neg_eval_split + non_song_eval_split)),\n",
    "            1: num_combined_train_samples / (2 * pos_eval_split)\n",
    "        }\n",
    "        \n",
    "        tf.keras.backend.clear_session()\n",
    "        tf.random.set_seed(settings.random_seed)\n",
    "\n",
    "        train_dataset = tf.data.Dataset.from_tensor_slices((train_data_x, train_data_y))\n",
    "        train_dataset = train_dataset.cache().shuffle(settings.buffer_size).batch(settings.batch_size)\n",
    "\n",
    "        eval_dataset = tf.data.Dataset.from_tensor_slices((eval_data_x, eval_data_y))\n",
    "        eval_dataset = eval_dataset.batch(settings.batch_size).repeat()\n",
    "\n",
    "        # Construct LSTM network\n",
    "        lstm_model = tf.keras.models.Sequential([\n",
    "            tf.keras.layers.LSTM(32, return_sequences=True, input_shape=train_data_x.shape[1:], dropout=0.05),\n",
    "            tf.keras.layers.LSTM(16, dropout=0.05),\n",
    "            tf.keras.layers.Dense(1, activation='sigmoid',\n",
    "                                  bias_initializer=tf.keras.initializers.Constant(\n",
    "                                      np.log([pos_eval_split/(neg_eval_split + non_song_eval_split)])))\n",
    "        ])\n",
    "\n",
    "        lstm_model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=variable_learning_rate(0)),\n",
    "                           loss='binary_crossentropy',\n",
    "                           metrics=[tf.keras.metrics.BinaryAccuracy()])\n",
    "\n",
    "        # Train\n",
    "        start_time = time.time()\n",
    "        history = lstm_model.fit(\n",
    "            x=train_dataset,\n",
    "            epochs=settings.epochs,\n",
    "            validation_data=eval_dataset,\n",
    "            validation_freq=settings.epochs_between_evals,\n",
    "            validation_steps=val_steps,\n",
    "            class_weight=class_weights,\n",
    "            initial_epoch=0,\n",
    "            shuffle=True,\n",
    "            callbacks=[tf.keras.callbacks.LearningRateScheduler(variable_learning_rate)],\n",
    "            verbose=2)\n",
    "        end_time = time.time()\n",
    "\n",
    "        # Save trained model. Reset metrics before saving\n",
    "        lstm_model.reset_metrics()\n",
    "        output_lstm_model_filename = get_lstm_model_filename(\n",
    "            lstm_type, lstm_exp['time_steps'], lstm_exp['pp'])\n",
    "        lstm_model.save(os.path.join(model_dir, output_lstm_model_filename))\n",
    "\n",
    "        # Free up some memory before next iteration\n",
    "        del lstm_model, train_dataset, eval_dataset, train_data_x, train_data_y, eval_data_x, eval_data_y\n",
    "\n",
    "        curr_training_time = end_time - start_time\n",
    "        if lstm_exp['time_steps'] in train_times:\n",
    "            train_times[lstm_exp['time_steps']].append(curr_training_time)\n",
    "        else:\n",
    "            train_times[lstm_exp['time_steps']] = [curr_training_time]\n",
    "        print('Training time: {}'.format(timedelta(seconds=curr_training_time)))\n",
    "\n",
    "print()\n",
    "print('================================================================================')\n",
    "print('Training times for hybrid type \"{:s}\":'.format(lstm_type))\n",
    "print('         : [Min., Max., Avg.]')\n",
    "\n",
    "for ts, tr_times in train_times.items():\n",
    "    min_time = timedelta(seconds=min(tr_times))\n",
    "    max_time = timedelta(seconds=max(tr_times))\n",
    "    avg_time = timedelta(seconds=sum(tr_times) / len(tr_times))\n",
    "    print(' {:s} TS{:3d}: [{}, {}, {}]'.format(lstm_type, ts, min_time, max_time, avg_time))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
